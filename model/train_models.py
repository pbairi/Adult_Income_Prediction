# -*- coding: utf-8 -*-
"""train_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VY_KnSifT37RdStHbMr0fdsjprahM_OG

# Problem Statement :
# The objective of this project is to predict whether an individual's annual income exceeds $50K based on demographic and employment-related attributes such as age, education, occupation, and working hours.

# Complete model training : train_mpdel.py
"""

# ==============================
# Import requoired libraries
# ==============================

import pandas as pd
import numpy as np
import joblib
import os

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score,
    roc_auc_score,
    precision_score,
    recall_score,
    f1_score,
    matthews_corrcoef
)

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier

from xgboost import XGBClassifier

import random
import warnings
warnings.filterwarnings('ignore')

# Kaggle credentials

os.environ['KAGGLE_USERNAME'] = 'praveenkumarbairi '
os.environ['KAGGLE_KEY']      = 'KGAT_2de7db02e0d8c410308d85c4f6e479ad'
assert os.environ['KAGGLE_USERNAME'] and os.environ['KAGGLE_KEY']
# Install Kaggle CLI
!pip -q install kaggle
!kaggle --version

# Install Dependancies

!pip install pandas numpy scikit-learn xgboost joblib

# ==============================
# Download Dataset
# ==============================

!mkdir -p model
!kaggle datasets download -d uciml/adult-census-income -p model --force

# Extract and reorganize into data folder
import zipfile, os, shutil
from pathlib import Path

zip_path = Path('model/adult-census-income.zip')
with zipfile.ZipFile(zip_path, 'r') as z:
    z.extractall('model')

#printing Data Set Counts

data = pd.read_csv("model/adult.csv")

print("✓ Adult Income - Dataset loaded successfully")

print(f"Dataset shape: {data.shape}")

print(f"Features: {list(data.columns)}")

print(f"\nFirst few rows:")
print(data.head())

# Basic statistics
print(f"\nDataset Statistics:")
print(data.describe())

# Only proceed with data preprocessing if the DataFrame was loaded successfully

# ==============================
# 2️⃣ Data Preprocessing
# ==============================

df = data.copy() # Initialize df with the loaded data

# Replace '?' and ' ?' with NaN
df.replace(["?", " ?"], np.nan, inplace=True)

# Drop missing values
df.dropna(inplace=True)

# Strip whitespace from object columns
for col in df.select_dtypes(include="object").columns:
    df[col] = df[col].str.strip()

# Encode target variable
df['income'] = df['income'].map({"<=50K": 0, ">50K": 1})

# Separate features and target
X = df.drop("income", axis=1)
y = df["income"]

# One-hot encoding
X = pd.get_dummies(X, drop_first=True)

# ==============================
# 3️⃣ Train-Test Split
# ==============================

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# =========================================
# 6️⃣ Create Test CSV for Streamlit Upload
# =========================================

# Use original test split (before scaling)
test_data = X_test.copy()
test_data["income"] = y_test.values

# Take 100 samples for lightweight Streamlit upload
test_sample = test_data.sample(n=100, random_state=42)

# Save CSV
test_sample.to_csv("model/test_data.csv", index=False)

print("\nTest data file 'test_data.csv' created successfully!")

# ==============================
# 4️⃣ Feature Scaling
# ==============================

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create model folder
if not os.path.exists("model"):
    os.makedirs("model")

# Save scaler
joblib.dump(scaler, "model/scaler.pkl")

# ==============================
# 5️⃣ Model Dictionary
# ==============================

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(max_depth=10, random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "Naive Bayes": GaussianNB(),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(
        use_label_encoder=False,
        eval_metric="logloss",
        random_state=42
    )
}

# ==============================
# 6️⃣ Training & Evaluation
# ==============================

results = []

for name, model in models.items():

    print(f"\nTraining {name}...")

    # Use scaled data only where required
    if name in ["Logistic Regression", "KNN", "Naive Bayes"]:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        y_prob = model.predict_proba(X_test_scaled)[:, 1]
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1]

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_prob)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    mcc = matthews_corrcoef(y_test, y_pred)

    results.append([
        name,
        round(accuracy, 4),
        round(auc, 4),
        round(precision, 4),
        round(recall, 4),
        round(f1, 4),
        round(mcc, 4)
    ])

    # Save model
    filename = f"model/{name.replace(' ', '_').lower()}.pkl"
    joblib.dump(model, filename)



# ==============================
# 7️⃣ Display Results
# ==============================

results_df = pd.DataFrame(
    results,
    columns=["Model", "Accuracy", "AUC", "Precision", "Recall", "F1", "MCC"]
)

print("\n==================================")
print("  Model Score Comparison Results")
print("==================================")
print(results_df)

# Save results to CSV
results_df.to_csv("model/model_results.csv", index=False)

